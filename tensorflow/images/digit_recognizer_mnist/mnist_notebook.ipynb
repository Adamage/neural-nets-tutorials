{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tools import Tools, SimpleMnistTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines hyperparameters. In simple terms, these are the numbers on which the training depends very much, and its hard to predict just how much. Especially: learning rate, which defines how 'far' should gradients be used to shift vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "TRAINING_ITERATIONS = 2500\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "VALIDATION_SIZE = 2000\n",
    "IMAGE_TO_DISPLAY = 7\n",
    "LOCAL_PATH = 'data'\n",
    "TRAIN_DATA = os.path.join(LOCAL_PATH, \"train.csv\")\n",
    "TEST_DATA = os.path.join(LOCAL_PATH, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0       0    ...            0         0         0         0         0   \n1       0    ...            0         0         0         0         0   \n2       0    ...            0         0         0         0         0   \n3       0    ...            0         0         0         0         0   \n4       0    ...            0         0         0         0         0   \n\n   pixel779  pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0         0  \n1         0         0         0         0         0  \n2         0         0         0         0         0  \n3         0         0         0         0         0  \n4         0         0         0         0         0  \n\n[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(TRAIN_DATA)\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = data.iloc[:, 1:].values\n",
    "images = images.astype(np.float)\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "image_size = images.shape[1]\n",
    "Tools.display(images[33], 28, 28)\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_flat = data[[0]].values.ravel()\n",
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "labels = Tools.dense_to_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin defining all variables involved in computational graph. The SimpleMnistTrainer is a small class in tools.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', shape=[None, image_size])\n",
    "y_labels = tf.placeholder('float', shape=[None, labels_count])\n",
    "trainer = SimpleMnistTrainer()\n",
    "trainer.train_images = train_images\n",
    "trainer.train_labels = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weights and biases in first layer. Define RELU (rectifined linear) 2D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = Tools.weight_variable(shape=[5, 5, 1, 32], name=\"Layer1_weights\")\n",
    "b_conv1 = Tools.bias_variable([32], name=\"Layer1_biases\")\n",
    "image = tf.reshape(x, [-1, image_width, image_height, 1])\n",
    "h_conv1 = tf.nn.relu(Tools.convolution2d(image, W_conv1) + b_conv1)\n",
    "h_pool1 = Tools.max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to make a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 32 features in 4 by 8 grid\n",
    "layer1 = tf.reshape(h_conv1, (-1, image_height, image_width, 4, 8))\n",
    "\n",
    "# reorder: channels, x, y\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4, 2))\n",
    "layer1 = tf.reshape(tensor=layer1, shape=(int(image_height) * 4, int(image_width * 8)), name=\"Layer1Reshape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weights and biases in second layer. Define RELU (rectifined linear) 2D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = Tools.weight_variable([5, 5, 32, 64], name=\"Layer2_weights\")\n",
    "b_conv2 = Tools.bias_variable([64], name=\"Layer2_biases\")\n",
    "h_conv2 = tf.nn.relu(Tools.convolution2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = Tools.max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 64 features in 4 by 16 grid\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4, 16))\n",
    "\n",
    "# reorder so the channels are in the first dimension, x and y follow.\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4, 2))\n",
    "layer2 = tf.reshape(layer2, (-1, 14 * 4, 14 * 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define third layer - densely connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = Tools.weight_variable([7 * 7 * 64, 1024], name=\"Layer3_fc_weights\")\n",
    "b_fc1 = Tools.bias_variable([1024], name=\"Layer3_fc_biases\")\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional algorithms. To prevent overfitting, use drop-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final layer is the output layer, a 'softmax', to convert output to one of 10 classes, for 10 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = Tools.weight_variable([1024, labels_count], name=\"Layer4_softmax_weights\")\n",
    "b_fc2 = Tools.bias_variable([labels_count], name=\"Layer4_softmax_biases\")\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having all the layers defined, now we specify how we are going to train the network. Define cost function, optimisation for gradient descent, and how to evaluate accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_labels * tf.log(y))\n",
    "# optimisation function\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "# evaluate\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "predict = tf.argmax(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 40000\n"
     ]
    }
   ],
   "source": [
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "display_step = 10\n",
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "trainer.number_of_examples = num_examples\n",
    "print(\"Number of examples: \" + str(num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save our model periodically, use Tensorflow checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "MODEL_PATH = os.path.join(LOCAL_PATH, \"model.ckpt\")\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"cost\", cross_entropy)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_operations = tf.summary.merge_all()\n",
    "\n",
    "train_logs = os.path.join(LOCAL_PATH, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start a Tensorflow session. During this session, all variables are initiated and computational graph is created. To be abble to start training from a previously saved checkpoint, implement a simple check if a checkpoint already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy => 0.9795\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    log_writer = tf.summary.FileWriter(train_logs, graph=session.graph)\n",
    "    \n",
    "    # This part handles resuming from a checkpoint.\n",
    "    if os.path.isfile(MODEL_PATH + \".meta\"):\n",
    "        ckpt = tf.train.get_checkpoint_state(LOCAL_PATH)\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # Do if no checkpoints are found.\n",
    "    else:\n",
    "        for i in range(TRAINING_ITERATIONS):\n",
    "            \n",
    "            batch_xs, batch_ys = trainer.next_batch(BATCH_SIZE)\n",
    "            \n",
    "            if i % display_step == 0 or (i + 1) == TRAINING_ITERATIONS:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: batch_xs, y_labels: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "                validation_accuracy = accuracy.eval(feed_dict={x: validation_images[0:BATCH_SIZE],\n",
    "                                                               y_labels: validation_labels[0:BATCH_SIZE],\n",
    "                                                               keep_prob: 1.0})\n",
    "                \n",
    "                #print('training_accuracy: {0} | validation_accuracy : {1} |  for step {2}'\n",
    "                #      .format(train_accuracy, validation_accuracy, i))\n",
    "                \n",
    "                validation_accuracies.append(validation_accuracy)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "                x_range.append(i)\n",
    "                \n",
    "                # Increase display_step.\n",
    "                if i % (display_step * 10) == 0 and i:\n",
    "                    display_step *= 10\n",
    "\n",
    "            summary, _ = session.run([summary_operations, train_step], \n",
    "                                     feed_dict={x: batch_xs, y_labels: batch_ys, keep_prob: DROPOUT})\n",
    "            \n",
    "            log_writer.add_summary(summary, global_step=i)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                saver.save(session, MODEL_PATH, global_step=i)\n",
    "\n",
    "        # Train loop finished. Check final accuracy on validation set\n",
    "        validation_accuracy = accuracy.eval(feed_dict={x: validation_images,\n",
    "                                                       y_labels: validation_labels,\n",
    "                                                       keep_prob: 1.0})\n",
    "        print('validation_accuracy => %.4f' % validation_accuracy)\n",
    "        saver.save(session, MODEL_PATH, global_step=TRAINING_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAABbCAYAAAC20GgMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nycTMzGr2//vCrMjLah5eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19e2926dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_range, train_accuracies, '-b', label='Training')\n",
    "plt.plot(x_range, validation_accuracies, '-g', label='Validation')\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.ylim(ymax=1.1, ymin=0.7)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model. Load test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pd.read_csv(TEST_DATA).values\n",
    "test_images = test_images.astype(np.float)\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "\n",
    "predicted_labels = np.zeros(test_images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ce7806bd1b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predicted_labels({0})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \"\"\"\n\u001b[1;32m--> 569\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3725\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   3728\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # This part handles resuming from a checkpoint.\n",
    "    ckpt = tf.train.get_checkpoint_state(LOCAL_PATH)\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)\n",
    "\n",
    "    for i in range(0, test_images.shape[0] // BATCH_SIZE):\n",
    "        if i % 10 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "        predicted_labels[i * BATCH_SIZE: (i + 1) * BATCH_SIZE] = \\\n",
    "            predict.eval(feed_dict={x: test_images[i * BATCH_SIZE: (i + 1) * BATCH_SIZE], keep_prob: 1.0})\n",
    "\n",
    "    print('predicted_labels({0})'.format(len(predicted_labels)))\n",
    "    \n",
    "    # Show test image and prediction\n",
    "    Tools.display(test_images[IMAGE_TO_DISPLAY], image_width, image_height)\n",
    "    print('predicted_labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY, predicted_labels[IMAGE_TO_DISPLAY]))\n",
    "    \n",
    "    test_images = StandardScaler().fit_transform(np.float32(test_images.values))\n",
    "    test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "    layer1_grid = layer1.eval(feed_dict={x: test_images[IMAGE_TO_DISPLAY:IMAGE_TO_DISPLAY + 1], keep_prob: 1.0})\n",
    "    plt.imshow(test_images[IMAGE_TO_DISPLAY, :, :, 0])\n",
    "    plt.show()\n",
    "    print(predicted_labels[IMAGE_TO_DISPLAY])\n",
    "    \n",
    "    layer2_grid = layer2.eval(feed_dict={x: test_images[IMAGE_TO_DISPLAY:IMAGE_TO_DISPLAY + 1], keep_prob: 1.0})\n",
    "    plt.imshow(layer2_grid[0])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}